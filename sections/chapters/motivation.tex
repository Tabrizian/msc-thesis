\chapter{Motivation}
\label{chap:motivation}

In this chapter we will discuss the unique characteristics of deep learning
jobs that make them suitable for co-location. We will discuss why deep learning
job are not usually able to saturate GPU resources in today's data centers.

\section{Deep Learning Job Characteristics}
\subsection{Heterogeneity of GPU Clusters}

\label{sec:hetrogenity}
GPU data centers are becoming more diverse with the introduction of new GPUs
every year. The GPUs that are available in the data centers have many
different capabilities. Some GPUs may have a couple of gigabytes of memory
while others may have tens of gigabytes of memory. At the time of writing
this thesis, A100 GPUs can support 80GBs~\cite{A10080gb} of memory. The
amount of memory available on a GPU affects the batch size that the developer
can choose. Larger batch sizes usually increase the utilization of a GPU. It
may seem like an obvious choice to divide the GPU memory by batch size to
fully utilize the GPU. However, not all the GPUs available in the cluster are
A100 gpus with 80 GBs of memory. In fact, these newer GPUs are more scarce
than the older GPUs due to the high demand and better performance.
Users usually end up selecting a larger spectrum of GPUs in order to reduce
the queuing time for their job to be scheduled. Since users cannot determine
the effective batch size for their job when they are submitting the job to
the cluster, they will end-up under utilizing the GPU and choosing smaller
batch size.
[Put a figure about increasing batch size and training speed]

\subsection{Statistical Constraints}
Training a deep neural network involves launching many similar jobs with a
single parameter change in order to find the best set of hyperparameters that
achieves the best performance. One may argue that the batch size problem
discussed in \cref{sec:hetrogenity} can be solved by querying the amount of
GPU memory available by size of memory required for a single batch of data at
the time the job is being scheduled. Since many different training jobs are
launched simultaneously, the hyperparameters found for one batch size, may be
different from the hyperparameters for the other job. Because of this, batch
size for all the jobs that training the same model should be the same.

Another issue is that some machine learning tasks are not able to use large
batch sizes, and large batch sizes come with diminished returns~\cite{Crossbow,DimmWitted}.
This constraint further leaves the GPUs underutilized.

\subsection{Monitoring, Logging, and Data Loading}
Training a neural network does not only involve GPU computation. A common
training task involves loading the data from the disk to the RAM, logging
metrics such as accuracy to monitor the progress of training~\cite{mlfow},
and check pointing the trained model so that the progress is not lost in case
of job preemption or power shutdown. Job preemption is implemented every hour
in some academic clusters such as Vector Institute. Preemption helps
avoiding starvation by giving everyone a fair share of the cluster. These
tasks lead to leaving the GPU unutilized for many cycles. Using co-location,
one job can use the GPU while the other job is busy monitoring metrics or
check pointing.
[Put a figure about PyTorch hello world CPU task]

\section{GPU Memory and Compute Capabilities}

\begin{figure}
    \centering
    \includegraphics[width=0.9\textwidth]{figs/gpus-overtime.pdf}
    \caption{GPU Capabilities over time}
    \label{fig:gpu-overtime}
\end{figure}

GPUs memory bandwidth and compute capabilities have increased by a tremendous
amount over time. \Cref{fig:gpu-overtime} shows this trend in the Tesla GPU
class. Tesla GPUs are NVIDIA's data center GPU class. The most recent data
center GPU class as of writing this thesis is the A100 GPU with the ability
to perform 312 TFLOPS half-precision operations. These increases in the compute
and memory capabilities have made it harder for application developers to fully
saturate the GPU resources. In this thesis, we use co-location to better
utilize GPUs even if they do not fully utilize the GPU individually.

\section{Summary}
In this chapter we discussed why co-location of jobs can be beneficial when
designing schedulers for deep learning jobs. We talked about the unique
characteristics of deep learning jobs and why they may not be able to fully
utilize the resources in the cluster. These factors include the statistical
constraints of the jobs, monitoring, logging or other CPU based tasks, and
heterogeneity of GPU clusters. In the next chapter, we provide detailed
profiling results to quantify the amount of speed up that can be gained
using co-location.
