\chapter{Motivation}
\label{chap:motivation}

Production GPU clusters suffer from low GPU utilization~\cite{Philly}. In a
GPU cluster trace published by Microsoft more than 40\% of the jobs only
utilize 20\% of the GPUs. In this chapter we discuss the potential reasons
behind why the GPU utilization is low in production GPU clusters and how
co-location can improve the utilization.

\section{GPU Cluster Characteristics}
\subsection{Heterogeneity of GPU Clusters}

\label{sec:hetrogenity}
GPU data centers are becoming more diverse with the introduction of new GPUs
every year. The GPUs that are available in the data centers have many
different capabilities. Some GPUs may have a couple of gigabytes of memory
while others may have tens of gigabytes of memory. At the time of writing
this thesis, A100 GPUs can support 80GBs~\cite{A10080gb} of memory. The
amount of memory available on a GPU affects the batch size that the developer
can choose. Larger batch sizes usually increase the utilization of a GPU. It
may seem like an obvious choice to divide the GPU memory by batch size to
fully utilize the GPU. However, not all the GPUs available in the cluster are
A100 GPUs with 80 GBs of memory. In fact, these newer GPUs are in greater
demand. Users usually end up selecting a larger spectrum of GPUs in order to
reduce the queuing time for their job to be scheduled. Since users cannot
determine the effective batch size for their job when they are submitting the
job to the cluster, they will end-up under utilizing the GPU and choosing
smaller batch size.

\begin{figure}
    \centering
    \includegraphics[width=0.9\textwidth]{figs/experiments/image-throughput}
    \caption{Batch Size vs Throughput}
    \label{fig:throughput}
\end{figure}

\Cref{fig:throughput} shows the throughput of different image classification
models. As we increase the batch size, the increase in throughput decreases.
This point is different for various models. The takeaway from this figure is
that apart from the GPU memory limit, the models see diminishing returns on
increasing the batch size for training.

\section{DL Job Characteristics}
\subsection{Statistical Constraints}
Training a deep neural network involves launching many similar jobs with a
single parameter change in order to find the best set of hyperparameters that
achieves the best performance. One may argue that the batch size problem
discussed in \cref{sec:hetrogenity} can be solved by querying the amount of
GPU memory available by size of memory required for a single batch of data at
the time the job is being scheduled. Since many different training jobs are
launched simultaneously, the hyperparameters found for one batch size, is 
different from the hyperparameters for the other job. Because of this, batch
size for all the jobs that training the same model should be the same.

Another issue is that some machine learning tasks are not able to use large
batch sizes, and large batch sizes come with diminishing
returns~\cite{Crossbow,DimmWitted}. This constraint further leaves the GPUs
underutilized.

\subsection{Monitoring, Logging, and Data Loading}
Training a neural network does not only involve GPU computation. A common
training task involves loading the data from the disk to the RAM, logging
metrics such as accuracy to monitor the progress of training~\cite{mlfow},
and check pointing the trained model so that the progress is not lost in case
of job preemption or power shutdown. Job preemption is implemented every hour
in some academic clusters such as Vector Institute. Preemption helps
avoiding starvation by giving everyone a fair share of the cluster. These
tasks lead to leaving the GPU unutilized for many cycles. Using co-location,
one job can use the GPU while the other job is busy monitoring metrics or
check pointing. This way the GPU is better utilized.
%[Put a figure about PyTorch hello world CPU task]

\section{GPU Memory and Compute Capabilities}

\begin{figure}
    \centering
    \includegraphics[width=0.9\textwidth]{figs/gpus-overtime.pdf}
    \caption{GPU Capabilities over time}
    \label{fig:gpu-overtime}
\end{figure}

GPU memory bandwidth and compute capabilities have increased by a tremendous
amount over time. \Cref{fig:gpu-overtime} shows this trend in the Tesla GPU
class. Tesla GPUs are NVIDIA's data center GPU class. The most recent data
center GPU class as of writing this thesis is the A100 GPU with the ability
to perform 312 TFLOPS half-precision operations. These increases in the compute
and memory capabilities have made it harder for application developers to fully
saturate the GPU resources. In this thesis, we use co-location to better
utilize GPUs even if the individual jobs cannot fully utilize the GPU.

\section{Summary}
In this chapter we discussed why co-location of jobs can be beneficial when
designing schedulers for deep learning jobs. We talked about the unique
characteristics of deep learning jobs and GPU clusters and why they may not
be able to fully utilize the resources in the cluster. These factors include
the statistical constraints of the jobs, monitoring, logging or other CPU
based tasks, and heterogeneity of GPU clusters. In the next chapter, we
provide detailed profiling results to quantify the amount of speed up that
can be gained using co-location.
