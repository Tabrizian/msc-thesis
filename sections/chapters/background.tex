\chapter{Background and Related Work}
\label{chap:background}

In this chapter we will discuss some background information related to this
work. We will discuss the basics of generic programming on GPUs and how
scheduling multiple processes work on GPUs. We also discuss the related
work to this thesis.

\section{Graphical Processing Units (GPUs)}

GPUs were among the first accelerators to be introduced as CPUs
co-processors. GPUs are throughput optimized in contrast to the CPUs which
are latency optimized. They consist of many inexpensive cores that can perform
a large number of operations in parallel. They also have a large memory
bandwidth that helps them bring data into these cores efficiently.

\begin{figure}
    \centering
    \includegraphics[width=0.4\textwidth]{figs/gpu}
    \caption{GPU Architecture}
    \label{fig:gpu-architecture}
\end{figure}
\Cref{fig:gpu-architecture} shows a simple illustration of the GPU
architecture. GPUs consist of several \textit{Streaming Multiprocessors
(SMs)}. SMs are the processing units in GPUs. Each of the SMs contains
various cores designed for operations on different data types. For example,
V100 GPU contains 84 SMs where each of them has 64 FP32 cores, 64 INT32
cores, 32 FP64 cores, and 8 tensor cores\cite{Volta}. In
\cref{fig:gpu-architecture}, \textit{CC} refers to the CUDA cores which
consists of all the cores present in each SM except the tensor cores. Tensor
Cores are abbreviated using \textit{TC}. They provide accelerated performance
for the reduced precision operations which are present in the Deep Learning
workloads. They were introduced in the Volta\cite{Volta} microarchitecture in
2017.

\subsection{Memory Hierarchy}
As shown in \cref{fig:gpu-architecture}, GPUs have a memory hierarchy
similar to the CPUs. The main difference is including a shared memory
that can be explicitly managed by the users program.


GPU memory hierarchy consists of:

\begin{itemize}
    \item \textbf{Registers}: registers are allocated to each individual thread.
    \item \textbf{L1 Cache}: L1 cache is shared among all the thread blocks in a SM.
    \item \textbf{Shared Memory}: Shared memory is similar to L1 cache except that it is explicitly managed by the user.
    \item \textbf{L2 Cache}: L2 cache is a large cached memory that is shared among all the SMs.
    \item \textbf{Global DRAM Memory}: It is the slowest memory access
    compared to all the other memory levels described above. This memory is very large (tens of GBs). Usually all
    the data required for a computation is stored in this memory.
\end{itemize}

\subsection{Programming Model}
\subsubsection{CUDA}
CUDA is a set of extensions to C/C++ to enable easier application development
in GPUs. CUDA also introduces a set of language abstractions that make it
easier to think about GPU programs. GPU accelerated programs use many threads
to perform the computation. CUDA groups the threads into \textit{thread
blocks} and \textit{grid blocks}. A \textit{thread block} is a group of
threads that are guaranteed to be running on the same SM. A \textit{grid
block} is a group of thread blocks that contain all the processing necessary
for the computation of a given \textit{kernel}. A \textit{kernel} is a
function that runs on a GPU. Both thread blocks and grid blocks can be
represented using three dimensions. \Cref{fig:threadblock} shows the difference
between a thread block, grid block, and a thread.

\begin{figure}
    \centering
    \includegraphics[width=0.9\textwidth]{figs/threadblock}
    \caption{Grid Block vs Thread Block vs Thread}
    \label{fig:threadblock}
\end{figure}

\subsubsection{Thread Block Scheduling}
There are various constraints that limit the scheduling of thread blocks into
the SMs. The number of registers, shared memory, and number of threads inside
a thread block are among the factors that limit the number of blocks that
can be scheduled into a single SM. Since there is a limited amount of these
resources available in each SM, the number of thread blocks will be limited
to the available resources. Apart from that, different GPU architectures 
have hard limits on the number of thread blocks and threads that can
be scheduled on a given SM. All these factors lead to a metric called
\textit{Theoretical Occupancy}\cite{Occupancy} of a kernel. Theoretical
Occupancy is a metric in percent that determines the percentage of active
warps in comparison with the total warps that could be scheduled on a given
GPU. \textit{warp} is a group of threads that are scheduled together. 
There is another concept called \textit{Achieved Occupancy}. Achieved
Occupancy measures the scheduled number of warps when the kernel is
actually running on the GPU. This can be different from the Theoretical
Occupancy because a given thread in a warp might be stalled on a memory load
and may not be ready to be scheduled. If there are not enough warps inflight
ready to be scheduled, the achieved occupancy will be lower than the
Theoretical Occupancy. Theoretical Occupancy
serves as the upper bound for the Achieved Occupancy.

\subsubsection{Life Cycle of a GPU accelerated Application}
\begin{figure}[h!]
    \centering
    \includegraphics[width=0.9\textwidth]{figs/cudalifecycle}
    \caption{Lifecycle of a GPU accelerated application}
    \label{fig:cudalifecycle}
\end{figure}

\Cref{fig:cudalifecycle} shows lifecycle of a typical CUDA application. The
application starts with allocating memory on the GPU. Then, it will copy
data from the host memory into the GPU memory. After that, the kernel
is executed. When the computation is complete, all the
results are copied back into the host memory. All these operations must be
executed inside a \textit{CUDA context}. Usually there is one CUDA context
associated with each process. In the \textit{Exclusive mode}, GPUs give
exclusive access to a single CUDA context but in the \textit{Default mode}
work submitted from multiple CUDA contexts to the GPU will be scheduled in a
time-sharing manner. MPS~\cite{MPS} allows multiple CUDA contexts to run
applications on the GPU concurrently. This is explained in more details
in \cref{sec:mps}.

\subsection{Concurrent Execution of Tasks on a GPU}
\subsubsection{CUDA Streams}
CUDA Stream is a software construct containing a series of commands that must
be executed in order. Work from different streams can be executed
concurrently. Recent GPUs are capable of executing work concurrently from
different CUDA streams belonging to the same CUDA context. Without
Multi-Process Service(MPS)~\cite{MPS}, it is not possible to run commands
from another CUDA context unless the work from the current CUDA context has
finished. A common use case for CUDA streams is overlapping computation and
communication to speedup the Kernel execution.

\subsubsection{Multi-Process Service (MPS)}
\label{sec:mps}
\begin{figure}
    \centering
    \includegraphics[width=0.9\textwidth]{figs/contextsscheduling}
    \caption{Interaction between CUDA contexts and Work Queues on a GPU}
    \label{fig:context-scheduling}
\end{figure}

MPS~\cite{MPS} is a mechanism that enables packing multiple processes
together without having to time-share the GPU. MPS achieves this by using a
client-server architecture. All the processes that want to run on the 
GPU are submitted to the MPS server. MPS is useful when an individual
job is not able to saturate all the GPU resources. Before Volta, MPS
could not isolate the memory address of different CUDA contexts running
on the same GPU. After Volta, MPS has improved the address space isolation
along with improved performance through hardware support for MPS.

\Cref{fig:context-scheduling} shows how CUDA contexts interact with the GPU
to schedule work. GPUs have a hardware construct named \textit{Work Queue}.
Different CUDA contexts cannot have their work be executed simultaneously
on the GPU. The GPU is time-shared between tasks coming from different
CUDA contexts.

\begin{figure}
    \centering
    \includegraphics[width=0.9\textwidth]{figs/mps}
    \caption{Interaction between CUDA contexts and Work Queues on a GPU}
    \label{fig:mps}
\end{figure}

\Cref{fig:mps} shows how CUDA contexts interact with the GPU when the MPS
server is running on the GPU. MPS server acts as a middle-man that intercepts
all the work that is being submitted to the GPU. MPS Server will then submit
the work on behalf of the application to the GPU. The GPU now will schedule
all the work from both of the CUDA contexts increasing the GPU utilization.

\section{Deep Learning}
Deep learning~\cite{deeplearning} is a machine learning paradigm that focuses
on training of artificial neural networks with many layers. Artificial Neural
Networks (ANNs) are function approximators that are proven to be
universal~\cite{universal}. It means that they can approximate any measurable
function to any desired degree of accuracy. This feature combined with the
advances in hardware and availability of large-scale datasets enabled
end-to-end learning algorithms that are able to generalize well in different
domains of Natural Language Processing~\cite{gpt}, Computer
Vision~\cite{alexnet}, and Speech Recognition.

\begin{figure}
    \centering
    \includegraphics[width=0.3\textwidth]{figs/feedforwardnn}
    \caption{A Fully Connected Feed Forward Neural Network}
    \label{fig:feedforwardnn}
\end{figure}

\Cref{fig:feedforwardnn} shows a simple illustration of a four-layer neural
network. In this figure, there are four layers in total. The first layer
contains the input data and the last layer is called the output layer. The
intermediate layers in this network are called the hidden layers. Each of the
nodes in these layers is called a neuron. Each neuron in every layer needs to
use a function for activation. The ultimate goal of a neural network is to
learn a set of weights that perform best on the test data set. The process
of learning the optimal weights is referred to as \textit{Training}. The
neural network starts with random weights. During each iteration of the
training a sample from the training data set is passed in the forward
direction known as the \textit{forward pass}. In the output layer, the neural
network compares the calculated output with the expected output and
calculates the difference between them using a metric known as \textit{loss}.
Loss is a scalar value, which will be propagated backward into the network
and the weights are updated for each layer. The updates for each layer is
calculated
by computing the partial derivatives with respect to the previous layer.
These updates along with a learning rate and an optimization algorithm will
determine the weights for the next iteration.

\subsection{GPUs and Deep Learning}
GPUs were originally designed for graphic processing. Because of the enormous
amount of vector computations that were required for graphics, GPUs were
invented. Deep learning also involves a variety of different operations. The
main operation is matrix by matrix multiplication which specifically GPUs are
very good at. Matrix by matrix multiplication is a compute-bound operation
which makes it the best fit for GPUs. Indeed, the peak compute performance of
GPUs is calculated by measuring the matrix multiplication. Although deep
learning techniques were introduced many decades ago, their feasibility
remained questioned until recent years. The hardware advances enabled these
computationally expensive workloads affordable and feasible in a timely
manner.

\section{Machine Learning Frameworks}

Machine Learning (ML) frameworks where introduced to enable easier adaption of new
deep learning techniques and help with democratization of research in this
area. The mainstream frameworks are specifically designed to help with
adoption of deep learning on specialized hardwares such GPUs or TPUs. These
frameworks are usually implemented in C++ for better performance with APIs in
the languages that the community prefers like Python. Designing a deep
learning framework is a very challenging task. ML frameworks have to adapt
very quickly with the latest versions of the accelerators. New accelerators
are announced around every year. With multiple accelerators present in this field,
making sure that you have the best performing algorithm on the new hardware
is not easy. Also, there are new models being introduced on a daily
basis. Making sure that all the state-of-the-art models produce the same
result on this large spectrum of hardware and software versions is an almost
impossible task. Because of this, mainstream frameworks are usually a result
of collaboration between hardware vendors and big software companies.

\subsection{Tensorflow}
\label{subsec:tensorflow}
Tensorflow~\cite{Tensorflow} is one of the first mainstream ML frameworks
open sourced by Google. Tensorflow uses a \textit{computation graph} to
describe all the computation necessary to achieve a task. When using a
computation graph, the user has to specify all the operations that it needs
beforehand. After that, they can compile the computation graph and get the
results. This model allows more optimizations to be performed during the
compile time. However, this model may limit the users ability to debug the
values when creating the model since the intermediate values are not
available. To fix this problem, Tensorflow introduced an eager execution
mode that uses an imperative model to make it easier to debug the models.
This model is usually slower as it doesn't have the ability to have an
omniscience view of the whole computation to perform the optimizations.

\subsection{PyTorch}
\label{subsec:pytorch}
PyTorch~\cite{PyTorch} introduced in 2016 by Facebook with the focus on
developer productivity. PyTorch uses \textit{autograd} to automatically store the
necessary information for calculating the gradient of arbitrary PyTorch code.
\textit{Autograd} is PyTorch's automattic differentiation engine that allows
to easily calculate the derivative of different sections of Python code.
Also, instead of using a computation graph, PyTorch uses an imperative
programming model that increases the developer productivity. Some research
frameworks~\cite{Janus} have also taken a hybrid approach to maintain both a
high developer productivity and an optimized, fast execution.

\subsection{MXNet}
MXNet~\cite{MXNet} was introduced in 2015. MXNet is an Apache project
supported by many universities and companies. MXNet tries to support both of
the programming paradigms discussed in
\cref{subsec:pytorch,subsec:tensorflow}. MXNet also supports a
\textit{hybrid} approach similar to Janus~\cite{Janus}.

\section{Related Work}
\label{chap:related}

\subsection{GPU Scheduling}

In the area of GPU schedulers for deep learning workloads there are various
related works to the work presented in this thesis. Optimus~\cite{Optimus} is
one of the earlier works in this area. The main goal of this work is which
job should be given more workers so that the total job completion time for a
set of jobs is minimized. It is assumed that the jobs use the Parameter
Server~\cite{ParameterServer} architecture. They learn the convergence curve
for a given job to predict the number of required iterations until the
completion. Using this information, they can measure how long the job is
going to last. Then, they create a heuristic algorithm that increases the
number of workers or parameter servers for a job that gains more speed up
compared to the other jobs. This work is complementary to our work. We can
augment their techniques with our co-location algorithms to better utilize
the GPUs.

Tiresias~\cite{Tiresias} presented a more realistic scheduling algorithm.
Tiresias is able to use the historical data of the jobs to minimize the job
completion time. They do not adaptively change the number of workers for a
given job. This is a more realistic approach since increasing the number of
workers may require retuning all the hyperparameters and may affect the
accuracy. They do not consider the packing of multiple jobs on the same GPU.

\begin{figure}
    \centering
    \includegraphics[width=0.9\textwidth]{figs/nnmemory}
    \caption{Memory Allocation in Forward and Backward Pass}
    \label{fig:memory-allocation-frameworks}
\end{figure}

\begin{equation}
    w(t+1) = w(t) - \eta * \nabla Q(w(t))
    \label{eq:sgd}
\end{equation}

Gandiva~\cite{Gandiva} introduced a fast context switch mechanism to avoid
starvation of the jobs in deep learning clusters. They observed that the GPU
memory usage of a job is not constant during the training and is minimum
between the iterations. \Cref{fig:memory-allocation-frameworks} shows how memory
allocation is performed during the training. In this figure, we are assuming
the training of a convolutional neural network~\cite{CNN}. The weights
associated with each layer are always present in the GPU. As the input
traverses different layers of the neural network, it creates
\textit{activations}. Activations must be stored for each layer. The
activations are required for calculating the gradients in \cref{eq:sgd}. \Cref{eq:sgd}
presents the updates for each iteration.  Represents the gradient
w.r.t. the current state of the model is represented using $\nabla Q(w(t))$.
Learning rate is represented using $\eta$.

These gradients will be calculated during the backward pass. In the backward
pass, the activation values can be discarded and as a consequence the memory
allocated for each of the layers may be freed. Gandiva leverages this pattern,
and does not interrupt the job during the forward or backward passes to reduce
the amount of data that needs to be copied during the checkpointing process.
Gandiva also included a mechanism for ``packing" the jobs to reduce the queuing
time and improve JCT. They employed random packing to find the matching job
pairs. As mentioned in~\cite{Gavel}, this strategy is not sufficient for finding
beneficial co-locations.

Chic~\cite{Chic} introduced the idea of adaptively changing the number of workers
using a reinforcement learning algorithm. They showed that using reinforcement
learning will lead to better results compared to Optimus~\cite{Optimus}. However,
they still didn't consider the packing of multiple jobs into a single GPU.

Gavel~\cite{Gavel} was the first scheduler to design a scheduling mechanism
that can use many different scheduling objectives. Gavel has support for
hierarchical and multi-domain scheduling policies. Their modeling of the
scheduling problem is able to take into account packing of multiple jobs
into a single GPU if the appropriate profiling information is available.
They also include a throughput estimator that is able to estimate the 
co-location throughput of unseen jobs.

\subsection{GPU Sharing}
Salus~\cite{Salus} provided a fast context switch mechanism by removing the
need for checkpointing. Similar to Gandiva~\cite{Gandiva}, they utilize
DL job GPU memory allocation patterns to enable faster context switch.
They divide the memory used by the deep learning frameworks into three types.
Ephemeral memory which is allocated and deallocated at every iteration. Model
memory which is the weights used by the model. The model memory is allocated
and deallocated once during the training. And the memory required by the
framework. This is usually smaller than the two other types of memory
allocations. Using their library, multiple processes can run on the same
GPU. They use ``GPU Lane" which is a logical component of Salus that tasks
are dispatched to it. GPU lanes are implemented using CUDA streams. They also
compared their implementation with MPS and achieved higher performance.
However, their comparisons were based on the Pascal MPS which is not
the MPS version that we used in this work. Volta MPS comes with greater
hardware support and fixes some of the errors that was mentioned in the
paper.

%PiepSwitch

\subsection{Jobs Interference}
Although interference of co-located DL jobs in GPU clusters has not been well 
explored, there is a significant body of work on the interference effect of
co-located jobs in CPU clusters. Quasar~\cite{Quasar} employs
PQ-reconstruction with Stochastic Gradient Descent, to fill the unspecified
elements of a matrix. In this matrix, the rows are jobs and the columns are
different platforms. Quasar first profiles a couple of jobs extensively on a
number of platforms. For unseen jobs, it profiles the job on a limited set of
platforms and then uses the PQ-reconstruction to predict the performance on
unseen platforms. Gavel~\cite{Gavel} used this technique in the ``Throughput
Estimator" to predict the performance of co-location. They treated the
co-located jobs as a new job and tried to fill in the matrix appropriately.

\subsection{Performance Models for GPU Multitasking}
HSM~\cite{HSM} provides a hybrid slowdown model that tries to predict the
performance of the kernels using a hybrid model. The term ``hybrid" here
refers to using both whitebox and blackbox models. For the compute-bound
models increasing the number of SMs has a linear corelation with the speedup
while that is not the case for memory-bound applications. The goal
of this paper was to implement the QoS guarantees using these models.
HSM was implemented in the GPUSim~\cite{GPUSIM}, a cycle simulator for
GPUs.

G-SDMS\cite{CUDAStream} presented a performance model for CUDA streams for the
compute-bound applications. The goal of this paper was to analyse the
scheduling of the thread blocks on SMs when CUDA streams are enabled. They
ultimately use these performance models to build a database that uses CUDA
streams to increase the performance.

\subsection{Scheduling CPUs in Big Data Clusters}
The problems in CPU scheduling focus on different requirements for scheduling.
The focus of the works in this area is mainly designing custom schedulers for
Hadoop~\cite{Hadoop} and Spark~\cite{Spark} clusters. The tasks on these
clusters are represented by a computation graph similar to DNNs. The
difference with the DL jobs is that the data is usually dispersed into
multiple nodes. The problem here is how we should decide on the scheduling of
each of the subtasks on each node given the data distribution on the nodes in
the cluster. \cite{DelayScheduling} adds a small amount of delay between
the scheduling of different jobs. The benefit of this approach is that this
small delay will help with the availability of nodes where the data for a given
task is present. Since each task takes a small amount of time, simply waiting
a bit longer may help with the data locality of the tasks.

Since the latency requirements of scheduling are very important in these
clusters, there have been some research trying to decentralize the scheduling
decisions for smaller latency requirements. Sparrow~\cite{Sparrow} is one of
the projects that focused on designing a decentralized scheduler for these
clusters. They also provided a mathematical analysis of their scheduling
mechanism.

There has also been recent papers that use Reinforcement
Learning~\cite{Decima} techniques to make the scheduling decisions.

\section{Summary}
We provided an overview of the GPU computation and basics of running
CUDA accelerated applications on GPUs. We also discussed how running
multiple tasks on the same GPU work. Additionally, we discussed
some of the mainstream ML frameworks and what are some of the
programming paradigms in these frameworks.

While some of the previous work mentioned co-location of multiple jobs into
the same GPU as a way to increase the GPU utilization, they didn't focus on
the reasons behind why jobs interference exists and how can we can minimize
the interference that results from co-location of multiple jobs together.
This is the first work to study the reasons behind the interference of
multiple deep learning training jobs and provides metrics to measure the
speedup and identify promising combinations.
