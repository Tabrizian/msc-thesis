\chapter{Related Work}
\label{chap:related}

\section{GPU Scheduling}

In the area of GPU schedulers for deep learning workloads there are various related works to the work presented in this thesis. Optimus~\cite{Optimus} is
one of the earlier works in this area. The main goal of this work is which
job should be given more workers so that the total job completion time for a
set of jobs is minimized. It is assumed that the jobs use the Parameter
Server~\cite{ParameterServer} architecture. They learn the convergence curve
for a given job to predict the number of required iterations until the
completion. Using this information, they can measure how long the job is
going to last. Then, they create a heuristic algorithm that increases the
number of workers or parameter servers for a job that gains more speed up
compared to the other jobs. This work is complementary to our work. We can
augment their techniques with our co-location algorithms to better utilize
the GPUs.

Tiresias~\cite{Tiresias} presented a more realistic scheduling algorithm.
Tiresias is able to use the historical data of the jobs to minimize the job
completion time. They do not adaptively change the number of workers for a 
given job. This is a more realistic approach since increasing the number
of workers affects the accuracy and may require retuning all the
hyperparameters. They do not consider the packing of multiple jobs on the
same GPU.

\begin{figure}
    \centering
    \includegraphics[width=0.9\textwidth]{figs/nnmemory}
    \caption{Memory Allocation in Forward and Backward Pass}
    \label{fig:memory-allocation-frameworks}
\end{figure}

\begin{equation}
    w(t+1) = w(t) - \eta * \nabla Q(w(t))
    \label{eq:sgd}
\end{equation}

Gandiva~\cite{Gandiva} introduced a fast context switch mechanism to avoid
starvation of the jobs in deep learning clusters. They observed that the GPU
memory usage of a job is not constant during the training and is minimum
between the iterations. While this is not true for PyTorch and Tensorflow
frameworks, some frameworks like MXNet deallocate the memory when it is no
longer needed. \Cref{fig:memory-allocation-frameworks} shows how memory
allocation is performed during the training. In this figure, we are assuming
the training of a convolutional neural network~\cite{CNN}. The weights
associated with each layer is always present in the GPU. As the input
traverses different layers of the neural network, it creates
\textit{activations}. Activations must be stored for each layer. The
activations are required for calculating the gradients in \cref{eq:sgd}.
These gradients will be calculated during the backward pass. In the backward
pass, the activation values can be discarded and as a consequence the memory
allocated for each of the layers may be freed. Gandiva leverages this pattern,
and does not interrupt the job during the forward or backward passes to reduce
the amount of data that needs to be copied during the checkpointing process.
Gandiva also included a mechanism for "packing" the jobs to reduce the queuing
time and improve JCT. They employed random packing to find the matching job
pairs. As mentioned in~\cite{Gavel}, this strategy is not sufficient for finding
beneficial co-locations.

Chic~\cite{Chic} introduced the idea of adaptively changing the number of workers
using a reinforcement learning algorithm. They showed that using reinforcement
learning will lead to better results compared to Optimus~\cite{Optimus}. However,
they still didn't consider the packing of multiple jobs into a single GPU.

Gavel~\cite{Gavel} was the first scheduler to design an scheduling mechanism
that can use many different scheduling objectives. Gavel has support for
hierarchical and multi-domain scheduling policies. Their modeling of the
scheduling problem is able to take into account packing of multiple jobs
into a single GPU if the appropriate profiling information is available.
They also include a throughput estimator that is able to estimate the 
co-location throughput of unseen jobs.

\section{Jobs Interference}

Although interference of co-located jobs in GPU clusters has not been very
explored, there is a significant body of work on the interference effect of
co-located jobs in CPU clusters. Quasar~\cite{Quasar} employs
PQ-reconstruction with Stochastic Gradient Descent, to fill the unspecified
elements of a matrix. In this matrix, the rows are jobs and the columns are
different platforms. Quasar first profiles a couple of jobs extensively on a
number of platforms. For unseen jobs, it profiles the job on a limited set of
platforms and then uses the PQ-reconstruction to predict the performance on
unseen platforms. Gavel~\cite{Gavel} used this technique in the "Throughput
Estimator" to predict the performance of co-location. They treated the
co-located jobs as a new job and tried to fill in the matrix appropriately.

\section{Scheduling of Data Analytics Job}