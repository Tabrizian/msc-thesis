@article{TPU,
  title   = {Google supercharges machine learning tasks with TPU custom chip},
  author  = {Jouppi, Norm},
  journal = {Google Blog, May},
  volume  = {18},
  pages   = {1},
  year    = {2016}
}

@misc{Volta,
  title        = {Volta Architecutre},
  howpublished = {\url{https://images.nvidia.com/content/volta-architecture/pdf/volta-architecture-whitepaper.pdf}},
  note         = {(Accessed on 10/20/2020)}
}

@misc{Occupancy,
  author       = {},
  title        = {CUDA Warps and Occupancy},
  howpublished = {\url{https://on-demand.gputechconf.com/gtc-express/2011/presentations/cuda_webinars_WarpsAndOccupancy.pdf}},
  month        = {},
  year         = {},
  note         = {(Accessed on 11/07/2020)}
}

@inproceedings{Tiresias,
  author    = {Juncheng Gu and Mosharaf Chowdhury and Kang G. Shin and Yibo Zhu and Myeongjae Jeon and Junjie Qian and Hongqiang Liu and Chuanxiong Guo},
  title     = {Tiresias: A {GPU} Cluster Manager for Distributed Deep Learning},
  booktitle = {16th {USENIX} Symposium on Networked Systems Design and Implementation ({NSDI} 19)},
  year      = {2019},
  isbn      = {978-1-931971-49-2},
  address   = {Boston, MA},
  pages     = {485--500},
  url       = {https://www.usenix.org/conference/nsdi19/presentation/gu},
  publisher = {{USENIX} Association},
  month     = feb
}

@inproceedings{Optimus,
  author    = {Peng, Yanghua and Bao, Yixin and Chen, Yangrui and Wu, Chuan and Guo, Chuanxiong},
  title     = {Optimus: An Efficient Dynamic Resource Scheduler for Deep Learning Clusters},
  year      = {2018},
  isbn      = {9781450355841},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3190508.3190517},
  doi       = {10.1145/3190508.3190517},
  abstract  = {Deep learning workloads are common in today's production clusters due to the proliferation of deep learning driven AI services (e.g., speech recognition, machine translation). A deep learning training job is resource-intensive and time-consuming. Efficient resource scheduling is the key to the maximal performance of a deep learning cluster. Existing cluster schedulers are largely not tailored to deep learning jobs, and typically specifying a fixed amount of resources for each job, prohibiting high resource efficiency and job performance. This paper proposes Optimus, a customized job scheduler for deep learning clusters, which minimizes job training time based on online resource-performance models. Optimus uses online fitting to predict model convergence during training, and sets up performance models to accurately estimate training speed as a function of allocated resources in each job. Based on the models, a simple yet effective method is designed and used for dynamically allocating resources and placing deep learning tasks to minimize job completion time. We implement Optimus on top of Kubernetes, a cluster manager for container orchestration, and experiment on a deep learning cluster with 7 CPU servers and 6 GPU servers, running 9 training jobs using the MXNet framework. Results show that Optimus outperforms representative cluster schedulers by about 139% and 63% in terms of job completion time and makespan, respectively.},
  booktitle = {Proceedings of the Thirteenth EuroSys Conference},
  articleno = {3},
  numpages  = {14},
  keywords  = {deep learning, resource management},
  location  = {Porto, Portugal},
  series    = {EuroSys '18}
}

@inbook{SLAQ,
  author    = {Zhang, Haoyu and Stafman, Logan and Or, Andrew and Freedman, Michael J.},
  title     = {SLAQ: Quality-Driven Scheduling for Distributed Machine Learning},
  year      = {2017},
  isbn      = {9781450350280},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3127479.3127490},
  abstract  = {Training machine learning (ML) models with large datasets can incur significant resource contention on shared clusters. This training typically involves many iterations that continually improve the quality of the model. Yet in exploratory settings, better models can be obtained faster by directing resources to jobs with the most potential for improvement. We describe SLAQ, a cluster scheduling system for approximate ML training jobs that aims to maximize the overall job quality.When allocating cluster resources, SLAQ explores the quality-runtime trade-offs across multiple jobs to maximize system-wide quality improvement. To do so, SLAQ leverages the iterative nature of ML training algorithms, by collecting quality and resource usage information from concurrent jobs, and then generating highly-tailored quality-improvement predictions for future iterations. Experiments show that SLAQ achieves an average quality improvement of up to 73% and an average delay reduction of up to 44% on a large set of ML training jobs, compared to resource fairness schedulers.},
  booktitle = {Proceedings of the 2017 Symposium on Cloud Computing},
  pages     = {390–404},
  numpages  = {15}
}

@inproceedings{ParameterServer,
  author    = {Mu Li and David G. Andersen and Jun Woo Park and Alexander J. Smola and Amr Ahmed and Vanja Josifovski and James Long and Eugene J. Shekita and Bor-Yiing Su},
  title     = {Scaling Distributed Machine Learning with the Parameter Server},
  booktitle = {11th {USENIX} Symposium on Operating Systems Design and Implementation ({OSDI} 14)},
  year      = {2014},
  isbn      = { 978-1-931971-16-4},
  address   = {Broomfield, CO},
  pages     = {583--598},
  url       = {https://www.usenix.org/conference/osdi14/technical-sessions/presentation/li_mu},
  publisher = {{USENIX} Association},
  month     = oct
}

@inproceedings{Gandiva,
  author    = {Wencong Xiao and Romil Bhardwaj and Ramachandran Ramjee and Muthian Sivathanu and Nipun Kwatra and Zhenhua Han and Pratyush Patel and Xuan Peng and Hanyu Zhao and Quanlu Zhang and Fan Yang and Lidong Zhou},
  title     = {Gandiva: Introspective Cluster Scheduling for Deep Learning},
  booktitle = {13th {USENIX} Symposium on Operating Systems Design and Implementation ({OSDI} 18)},
  year      = {2018},
  isbn      = {978-1-939133-08-3},
  address   = {Carlsbad, CA},
  pages     = {595--610},
  url       = {https://www.usenix.org/conference/osdi18/presentation/xiao},
  publisher = {{USENIX} Association},
  month     = oct
}

@article{CNN,
  title     = {Backpropagation applied to handwritten zip code recognition},
  author    = {LeCun, Yann and Boser, Bernhard and Denker, John S and Henderson, Donnie and Howard, Richard E and Hubbard, Wayne and Jackel, Lawrence D},
  journal   = {Neural computation},
  volume    = {1},
  number    = {4},
  pages     = {541--551},
  year      = {1989},
  publisher = {MIT Press}
}

@inproceedings{Gavel,
  author    = {Deepak Narayanan and Keshav Santhanam and Fiodar Kazhamiaka and Amar Phanishayee and Matei Zaharia},
  title     = {Heterogeneity-Aware Cluster Scheduling Policies for Deep Learning Workloads},
  booktitle = {14th {USENIX} Symposium on Operating Systems Design and Implementation ({OSDI} 20)},
  year      = {2020},
  isbn      = {978-1-939133-19-9},
  pages     = {481--498},
  url       = {https://www.usenix.org/conference/osdi20/presentation/narayanan-deepak},
  publisher = {{USENIX} Association},
  month     = nov
}

@inproceedings{Chic,
  author    = {Gong, Yifan and Li, Baochun and Liang, Ben and Zhan, Zheng},
  title     = {Chic: Experience-Driven Scheduling in Machine Learning Clusters},
  year      = {2019},
  isbn      = {9781450367783},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3326285.3329065},
  doi       = {10.1145/3326285.3329065},
  abstract  = {Large-scale machine learning (ML) models are routinely trained in a distributed fashion, due to their increasing complexity and data sizes. In a shared cluster handling multiple distributed learning workloads with a parameter server framework, it is important to determine the adequate number of concurrent workers and parameter servers for each ML workload over time, in order to minimize the average completion time and increase resource utilization. Existing schedulers for machine learning workloads involve meticulously designed heuristics. However, as the execution environment is highly complex and dynamic, it is challenging to construct an accurate model to make online decisions. In this paper, we design an experience-driven approach that learns to manage the cluster directly from experience rather than using a mathematical model. We propose Chic, a scheduler that is tailored for scheduling machine learning workloads in a cluster by leveraging deep reinforcement learning techniques. With our design of the state space, action space, and reward function, Chic trains a deep neural network with a modified version of the cross-entropy method to approximate the policy for assigning workers and parameter servers for future workloads based on the experience of the agent. Furthermore, a simplified version named Chic-Pair with a shorter training time for the policy is purposed by assigning workers and parameter servers in a pair. We compare Chic and Pair with state-of-the-art heuristics, and our results show that Chic and Chic-Pair are able to reduce the average training time significantly for machine learning workloads under a wide variety of conditions.},
  booktitle = {Proceedings of the International Symposium on Quality of Service},
  articleno = {30},
  numpages  = {10},
  keywords  = {workload scheduling, deep reinforcement learning, distributed machine learning},
  location  = {Phoenix, Arizona},
  series    = {IWQoS '19}
}

@misc{MPS,
  author       = {},
  title        = {Multi-Process Service},
  howpublished = {\url{https://docs.nvidia.com/deploy/pdf/CUDA_Multi_Process_Service_Overview.pdf}},
  month        = {},
  year         = {},
  note         = {(Accessed on 11/16/2020)}
}

@inproceedings{10.1145/2541940.2541941,
author = {Delimitrou, Christina and Kozyrakis, Christos},
title = {Quasar: Resource-Efficient and QoS-Aware Cluster Management},
year = {2014},
isbn = {9781450323055},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2541940.2541941},
doi = {10.1145/2541940.2541941},
abstract = {Cloud computing promises flexibility and high performance for users and high cost-efficiency for operators. Nevertheless, most cloud facilities operate at very low utilization, hurting both cost effectiveness and future scalability.We present Quasar, a cluster management system that increases resource utilization while providing consistently high application performance. Quasar employs three techniques. First, it does not rely on resource reservations, which lead to underutilization as users do not necessarily understand workload dynamics and physical resource requirements of complex codebases. Instead, users express performance constraints for each workload, letting Quasar determine the right amount of resources to meet these constraints at any point. Second, Quasar uses classification techniques to quickly and accurately determine the impact of the amount of resources (scale-out and scale-up), type of resources, and interference on performance for each workload and dataset. Third, it uses the classification results to jointly perform resource allocation and assignment, quickly exploring the large space of options for an efficient way to pack workloads on available resources. Quasar monitors workload performance and adjusts resource allocation and assignment when needed. We evaluate Quasar over a wide range of workload scenarios, including combinations of distributed analytics frameworks and low-latency, stateful services, both on a local cluster and a cluster of dedicated EC2 servers. At steady state, Quasar improves resource utilization by 47% in the 200-server EC2 cluster, while meeting performance constraints for workloads of all types.},
booktitle = {Proceedings of the 19th International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {127–144},
numpages = {18},
keywords = {quality of service, resource allocation and assignment, resource efficiency, cloud computing, cluster management, datacenters},
location = {Salt Lake City, Utah, USA},
series = {ASPLOS '14}
}

@article{10.1145/2654822.2541941,
author = {Delimitrou, Christina and Kozyrakis, Christos},
title = {Quasar: Resource-Efficient and QoS-Aware Cluster Management},
year = {2014},
issue_date = {March 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {42},
number = {1},
issn = {0163-5964},
url = {https://doi.org/10.1145/2654822.2541941},
doi = {10.1145/2654822.2541941},
abstract = {Cloud computing promises flexibility and high performance for users and high cost-efficiency for operators. Nevertheless, most cloud facilities operate at very low utilization, hurting both cost effectiveness and future scalability.We present Quasar, a cluster management system that increases resource utilization while providing consistently high application performance. Quasar employs three techniques. First, it does not rely on resource reservations, which lead to underutilization as users do not necessarily understand workload dynamics and physical resource requirements of complex codebases. Instead, users express performance constraints for each workload, letting Quasar determine the right amount of resources to meet these constraints at any point. Second, Quasar uses classification techniques to quickly and accurately determine the impact of the amount of resources (scale-out and scale-up), type of resources, and interference on performance for each workload and dataset. Third, it uses the classification results to jointly perform resource allocation and assignment, quickly exploring the large space of options for an efficient way to pack workloads on available resources. Quasar monitors workload performance and adjusts resource allocation and assignment when needed. We evaluate Quasar over a wide range of workload scenarios, including combinations of distributed analytics frameworks and low-latency, stateful services, both on a local cluster and a cluster of dedicated EC2 servers. At steady state, Quasar improves resource utilization by 47% in the 200-server EC2 cluster, while meeting performance constraints for workloads of all types.},
journal = {SIGARCH Comput. Archit. News},
month = feb,
pages = {127–144},
numpages = {18},
keywords = {resource allocation and assignment, resource efficiency, cloud computing, cluster management, datacenters, quality of service}
}

@article{Quasar,
  author     = {Delimitrou, Christina and Kozyrakis, Christos},
  title      = {Quasar: Resource-Efficient and QoS-Aware Cluster Management},
  year       = {2014},
  issue_date = {April 2014},
  publisher  = {Association for Computing Machinery},
  address    = {New York, NY, USA},
  volume     = {49},
  number     = {4},
  issn       = {0362-1340},
  url        = {https://doi.org/10.1145/2644865.2541941},
  doi        = {10.1145/2644865.2541941},
  abstract   = {Cloud computing promises flexibility and high performance for users and high cost-efficiency for operators. Nevertheless, most cloud facilities operate at very low utilization, hurting both cost effectiveness and future scalability.We present Quasar, a cluster management system that increases resource utilization while providing consistently high application performance. Quasar employs three techniques. First, it does not rely on resource reservations, which lead to underutilization as users do not necessarily understand workload dynamics and physical resource requirements of complex codebases. Instead, users express performance constraints for each workload, letting Quasar determine the right amount of resources to meet these constraints at any point. Second, Quasar uses classification techniques to quickly and accurately determine the impact of the amount of resources (scale-out and scale-up), type of resources, and interference on performance for each workload and dataset. Third, it uses the classification results to jointly perform resource allocation and assignment, quickly exploring the large space of options for an efficient way to pack workloads on available resources. Quasar monitors workload performance and adjusts resource allocation and assignment when needed. We evaluate Quasar over a wide range of workload scenarios, including combinations of distributed analytics frameworks and low-latency, stateful services, both on a local cluster and a cluster of dedicated EC2 servers. At steady state, Quasar improves resource utilization by 47% in the 200-server EC2 cluster, while meeting performance constraints for workloads of all types.},
  journal    = {SIGPLAN Not.},
  month      = feb,
  pages      = {127–144},
  numpages   = {18},
  keywords   = {quality of service, cluster management, resource allocation and assignment, resource efficiency, cloud computing, datacenters}
}

@inproceedings{10.1145/2451116.2451125,
  author    = {Delimitrou, Christina and Kozyrakis, Christos},
  title     = {Paragon: QoS-Aware Scheduling for Heterogeneous Datacenters},
  year      = {2013},
  isbn      = {9781450318709},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/2451116.2451125},
  doi       = {10.1145/2451116.2451125},
  abstract  = {Large-scale datacenters (DCs) host tens of thousands of diverse applications each day. However, interference between colocated workloads and the difficulty to match applications to one of the many hardware platforms available can degrade performance, violating the quality of service (QoS) guarantees that many cloud workloads require. While previous work has identified the impact of heterogeneity and interference, existing solutions are computationally intensive, cannot be applied online and do not scale beyond few applications.We present Paragon, an online and scalable DC scheduler that is heterogeneity and interference-aware. Paragon is derived from robust analytical methods and instead of profiling each application in detail, it leverages information the system already has about applications it has previously seen. It uses collaborative filtering techniques to quickly and accurately classify an unknown, incoming workload with respect to heterogeneity and interference in multiple shared resources, by identifying similarities to previously scheduled applications. The classification allows Paragon to greedily schedule applications in a manner that minimizes interference and maximizes server utilization. Paragon scales to tens of thousands of servers with marginal scheduling overheads in terms of time or state.We evaluate Paragon with a wide range of workload scenarios, on both small and large-scale systems, including 1,000 servers on EC2. For a 2,500-workload scenario, Paragon enforces performance guarantees for 91% of applications, while significantly improving utilization. In comparison, heterogeneity-oblivious, interference-oblivious and least-loaded schedulers only provide similar guarantees for 14%, 11% and 3% of workloads. The differences are more striking in oversubscribed scenarios where resource efficiency is more critical.},
  booktitle = {Proceedings of the Eighteenth International Conference on Architectural Support for Programming Languages and Operating Systems},
  pages     = {77–88},
  numpages  = {12},
  keywords  = {heterogeneity, cloud computing, scheduling, interference, datacenter, qos},
  location  = {Houston, Texas, USA},
  series    = {ASPLOS '13}
}

@misc{A10080gb,
  author       = {},
  title        = {NVIDIA Doubles Down: Announces A100 80GB GPU, Supercharging World’s Most Powerful GPU for AI Supercomputing | NVIDIA Newsroom},
  howpublished = {\url{https://nvidianews.nvidia.com/news/nvidia-doubles-down-announces-a100-80gb-gpu-supercharging-worlds-most-powerful-gpu-for-ai-supercomputing}},
  month        = {},
  year         = {},
  note         = {(Accessed on 11/22/2020)}
}

@article{Crossbow,
  author     = {Koliousis, Alexandros and Watcharapichat, Pijika and Weidlich, Matthias and Mai, Luo and Costa, Paolo and Pietzuch, Peter},
  title      = {Crossbow: Scaling Deep Learning with Small Batch Sizes on Multi-GPU Servers},
  year       = {2019},
  issue_date = {July 2019},
  publisher  = {VLDB Endowment},
  volume     = {12},
  number     = {11},
  issn       = {2150-8097},
  url        = {https://doi.org/10.14778/3342263.3342276},
  doi        = {10.14778/3342263.3342276},
  abstract   = {Deep learning models are trained on servers with many GPUs, and training must scale with the number of GPUs. Systems such as TensorFlow and Caffe2 train models with parallel synchronous stochastic gradient descent: they process a batch of training data at a time, partitioned across GPUs, and average the resulting partial gradients to obtain an updated global model. To fully utilise all GPUs, systems must increase the batch size, which hinders statistical efficiency. Users tune hyper-parameters such as the learning rate to compensate for this, which is complex and model-specific.We describe Crossbow, a new single-server multi-GPU system for training deep learning models that enables users to freely choose their preferred batch size---however small---while scaling to multiple GPUs. Crossbow uses many parallel model replicas and avoids reduced statistical efficiency through a new synchronous training method. We introduce SMA, a synchronous variant of model averaging in which replicas independently explore the solution space with gradient descent, but adjust their search synchronously based on the trajectory of a globally-consistent average model. Crossbow achieves high hardware efficiency with small batch sizes by potentially training multiple model replicas per GPU, automatically tuning the number of replicas to maximise throughput. our experiments show that Crossbow improves the training time of deep learning models on an 8-GPU server by 1.3--4X compared to TensorFlow.},
  journal    = {Proc. VLDB Endow.},
  month      = jul,
  pages      = {1399–1412},
  numpages   = {14}
}

@article{DimmWitted,
  author     = {Zhang, Ce and R\'{e}, Christopher},
  title      = {DimmWitted: A Study of Main-Memory Statistical Analytics},
  year       = {2014},
  issue_date = {August 2014},
  publisher  = {VLDB Endowment},
  volume     = {7},
  number     = {12},
  issn       = {2150-8097},
  url        = {https://doi.org/10.14778/2732977.2733001},
  doi        = {10.14778/2732977.2733001},
  abstract   = {We perform the first study of the tradeoff space of access methods and replication to support statistical analytics using first-order methods executed in the main memory of a Non-Uniform Memory Access (NUMA) machine. Statistical analytics systems differ from conventional SQL-analytics in the amount and types of memory incoherence that they can tolerate. Our goal is to understand tradeoffs in accessing the data in row- or column-order and at what granularity one should share the model and data for a statistical task. We study this new tradeoff space and discover that there are tradeoffs between hardware and statistical efficiency. We argue that our tradeoff study may provide valuable information for designers of analytics engines: for each system we consider, our prototype engine can run at least one popular task at least 100\texttimes{} faster. We conduct our study across five architectures using popular models, including SVMs, logistic regression, Gibbs sampling, and neural networks.},
  journal    = {Proc. VLDB Endow.},
  month      = aug,
  pages      = {1283–1294},
  numpages   = {12}
}

@article{mlfow,
  title   = {Accelerating the Machine Learning Lifecycle with MLflow.},
  author  = {Zaharia, Matei and Chen, Andrew and Davidson, Aaron and Ghodsi, Ali and Hong, Sue Ann and Konwinski, Andy and Murching, Siddharth and Nykodym, Tomas and Ogilvie, Paul and Parkhe, Mani and others},
  journal = {IEEE Data Eng. Bull.},
  volume  = {41},
  number  = {4},
  pages   = {39--45},
  year    = {2018}
}

@inproceedings{resnet,
  title     = {Deep residual learning for image recognition},
  author    = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle = {Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages     = {770--778},
  year      = {2016}
}

@article{vgg,
  title   = {Very deep convolutional networks for large-scale image recognition},
  author  = {Simonyan, Karen and Zisserman, Andrew},
  journal = {arXiv preprint arXiv:1409.1556},
  year    = {2014}
}

@inproceedings{senet,
  title     = {Squeeze-and-excitation networks},
  author    = {Hu, Jie and Shen, Li and Sun, Gang},
  booktitle = {Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages     = {7132--7141},
  year      = {2018}
}

@inproceedings{Tensorflow,
  title     = {Tensorflow: A system for large-scale machine learning},
  author    = {Abadi, Mart{\'\i}n and Barham, Paul and Chen, Jianmin and Chen, Zhifeng and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Irving, Geoffrey and Isard, Michael and others},
  booktitle = {12th $\{$USENIX$\}$ symposium on operating systems design and implementation ($\{$OSDI$\}$ 16)},
  pages     = {265--283},
  year      = {2016}
}

@inproceedings{PyTorch,
  author    = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
  booktitle = {Advances in Neural Information Processing Systems},
  pages     = {8026--8037},
  publisher = {Curran Associates, Inc.},
  title     = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},
  url       = {https://proceedings.neurips.cc/paper/2019/file/bdbca288fee7f92f2bfa9f7012727740-Paper.pdf},
  volume    = {32},
  year      = {2019}
}

@inproceedings{Janus,
  author    = {Eunji Jeong and Sungwoo Cho and Gyeong-In Yu and Joo Seong Jeong and Dong-Jin Shin and Byung-Gon Chun},
  title     = {{JANUS}: Fast and Flexible Deep Learning via Symbolic Graph Execution of Imperative Programs},
  booktitle = {16th {USENIX} Symposium on Networked Systems Design and Implementation ({NSDI} 19)},
  year      = {2019},
  isbn      = {978-1-931971-49-2},
  address   = {Boston, MA},
  pages     = {453--468},
  url       = {https://www.usenix.org/conference/nsdi19/presentation/jeong},
  publisher = {{USENIX} Association},
  month     = feb
}

@article{MXNet,
  title   = {Mxnet: A flexible and efficient machine learning library for heterogeneous distributed systems},
  author  = {Chen, Tianqi and Li, Mu and Li, Yutian and Lin, Min and Wang, Naiyan and Wang, Minjie and Xiao, Tianjun and Xu, Bing and Zhang, Chiyuan and Zhang, Zheng},
  journal = {arXiv preprint arXiv:1512.01274},
  year    = {2015}
}

@misc{NsightCompute,
  author       = {},
  title        = {NVIDIA Nsight Compute | NVIDIA Developer},
  howpublished = {\url{https://developer.nvidia.com/nsight-compute}},
  month        = {},
  year         = {},
  note         = {(Accessed on 11/29/2020)}
}

@article{deeplearning,
  title     = {Deep learning},
  author    = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
  journal   = {nature},
  volume    = {521},
  number    = {7553},
  pages     = {436--444},
  year      = {2015},
  publisher = {Nature Publishing Group}
}

@article{universal,
  title   = {Multilayer feedforward networks are universal approximators.},
  author  = {Hornik, Kurt and Stinchcombe, Maxwell and White, Halbert and others},
  journal = {Neural networks},
  volume  = {2},
  number  = {5},
  pages   = {359--366},
  year    = {1989}
}

@article{gpt,
  title   = {Language models are few-shot learners},
  author  = {Brown, Tom B and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal = {arXiv preprint arXiv:2005.14165},
  year    = {2020}
}

@article{alexnet,
  title     = {Imagenet classification with deep convolutional neural networks},
  author    = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  journal   = {Communications of the ACM},
  volume    = {60},
  number    = {6},
  pages     = {84--90},
  year      = {2017},
  publisher = {AcM New York, NY, USA}
}

@inproceedings{PiepSwitch,
  author    = {Zhihao Bai and Zhen Zhang and Yibo Zhu and Xin Jin},
  title     = {PipeSwitch: Fast Pipelined Context Switching for Deep Learning Applications},
  booktitle = {14th {USENIX} Symposium on Operating Systems Design and Implementation ({OSDI} 20)},
  year      = {2020},
  isbn      = {978-1-939133-19-9},
  pages     = {499--514},
  url       = {https://www.usenix.org/conference/osdi20/presentation/bai},
  publisher = {{USENIX} Association},
  month     = nov
}

@article{Salus,
  title   = {Salus: Fine-grained gpu sharing primitives for deep learning applications},
  author  = {Yu, Peifeng and Chowdhury, Mosharaf},
  journal = {arXiv preprint arXiv:1902.04610},
  year    = {2019}
}

@inproceedings{Clipper,
  title     = {Clipper: A low-latency online prediction serving system},
  author    = {Crankshaw, Daniel and Wang, Xin and Zhou, Guilio and Franklin, Michael J and Gonzalez, Joseph E and Stoica, Ion},
  booktitle = {14th $\{$USENIX$\}$ Symposium on Networked Systems Design and Implementation ($\{$NSDI$\}$ 17)},
  pages     = {613--627},
  year      = {2017}
}

@misc{Ampere,
  author       = {},
  title        = {nvidia-ampere-architecture-whitepaper.pdf},
  howpublished = {\url{https://www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/nvidia-ampere-architecture-whitepaper.pdf}},
  month        = {},
  year         = {},
  note         = {(Accessed on 12/06/2020)}
}

@article{Kubernetes,
  title     = {Borg, omega, and kubernetes},
  author    = {Burns, Brendan and Grant, Brian and Oppenheimer, David and Brewer, Eric and Wilkes, John},
  journal   = {Queue},
  volume    = {14},
  number    = {1},
  pages     = {70--93},
  year      = {2016},
  publisher = {ACM New York, NY, USA}
}

@misc{Redis,
  author       = {},
  title        = {Redis},
  howpublished = {\url{https://redis.io/}},
  month        = {},
  year         = {},
  note         = {(Accessed on 12/19/2020)}
}

@misc{Flask,
  author       = {},
  title        = {Welcome to Flask — Flask Documentation (1.1.x)},
  howpublished = {\url{https://flask.palletsprojects.com/en/1.1.x/}},
  month        = {},
  year         = {},
  note         = {(Accessed on 12/20/2020)}
}

@article{scikit-learn,
 title={Scikit-learn: Machine Learning in {P}ython},
 author={Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V.
         and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P.
         and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and
         Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
 journal={Journal of Machine Learning Research},
 volume={12},
 pages={2825--2830},
 year={2011}
}

@article{CIFAR10,
  title={Learning multiple layers of features from tiny images},
  author={Krizhevsky, Alex and Hinton, Geoffrey and others},
  year={2009},
  publisher={Citeseer}
}

@inproceedings{Hypersched,
  title={Hypersched: Dynamic resource reallocation for model development on a deadline},
  author={Liaw, Richard and Bhardwaj, Romil and Dunlap, Lisa and Zou, Yitian and Gonzalez, Joseph E and Stoica, Ion and Tumanov, Alexey},
  booktitle={Proceedings of the ACM Symposium on Cloud Computing},
  pages={61--73},
  year={2019}
}

@article{ASHA,
  title={Massively parallel hyperparameter tuning},
  author={Li, Liam and Jamieson, Kevin and Rostamizadeh, Afshin and Gonina, Ekaterina and Hardt, Moritz and Recht, Benjamin and Talwalkar, Ameet},
  journal={arXiv preprint arXiv:1810.05934},
  year={2018}
}

@inproceedings{HSM,
  author    = {Zhao, Xia and Jahre, Magnus and Eeckhout, Lieven},
  title     = {HSM: A Hybrid Slowdown Model for Multitasking GPUs},
  year      = {2020},
  isbn      = {9781450371025},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3373376.3378457},
  doi       = {10.1145/3373376.3378457},
  abstract  = {Graphics Processing Units (GPUs) are increasingly widely used in the cloud to accelerate compute-heavy tasks. However, GPU-compute applications stress the GPU architecture in different ways --- leading to suboptimal resource utilization when a single GPU is used to run a single application. One solution is to use the GPU in a multitasking fashion to improve utilization. Unfortunately, multitasking leads to destructive interference between co-running applications which causes fairness issues and Quality-of-Service (QoS) violations.We propose the Hybrid Slowdown Model (HSM) to dynamically and accurately predict application slowdown due to interference. HSM overcomes the low accuracy of prior white-box models, and training and implementation overheads of pure black-box models, with a hybrid approach. More specifically, the white-box component of HSM builds upon the fundamental insight that effective bandwidth utilization is proportional to DRAM row buffer hit rate, and the black-box component of HSM uses linear regression to relate row buffer hit rate to performance. HSM accurately predicts application slowdown with an average error of 6.8%, a significant improvement over the current state-of-the-art. In addition, we use HSM to guide various resource management schemes in multitasking GPUs: HSM-Fair significantly improves fairness (by 1.59x on average) compared to even partitioning, whereas HSM-QoS improves system throughput (by 18.9% on average) compared to proportional SM partitioning while maintaining the QoS target for the high-priority application in challenging mixed memory/compute-bound multi-program workloads.},
  booktitle = {Proceedings of the Twenty-Fifth International Conference on Architectural Support for Programming Languages and Operating Systems},
  pages     = {1371–1385},
  numpages  = {15},
  keywords  = {gpu, multitasking, performance modeling, slowdown prediction},
  location  = {Lausanne, Switzerland},
  series    = {ASPLOS '20}
}

@inproceedings{GPUSIM,
  author    = {M. {Khairy} and Z. {Shen} and T. M. {Aamodt} and T. G. {Rogers}},
  booktitle = {2020 ACM/IEEE 47th Annual International Symposium on Computer Architecture (ISCA)},
  title     = {Accel-Sim: An Extensible Simulation Framework for Validated GPU Modeling},
  year      = {2020},
  volume    = {},
  number    = {},
  pages     = {473-486},
  doi       = {10.1109/ISCA45697.2020.00047}
}

@inproceedings{CUDAStream,
  title        = {Performance modeling in CUDA streams—A means for high-throughput data processing},
  author       = {Li, Hao and Yu, Di and Kumar, Anand and Tu, Yi-Cheng},
  booktitle    = {2014 IEEE International Conference on Big Data (Big Data)},
  pages        = {301--310},
  year         = {2014},
  organization = {IEEE}
}

@inproceedings{DelayScheduling,
  author    = {Zaharia, Matei and Borthakur, Dhruba and Sen Sarma, Joydeep and Elmeleegy, Khaled and Shenker, Scott and Stoica, Ion},
  title     = {Delay Scheduling: A Simple Technique for Achieving Locality and Fairness in Cluster Scheduling},
  year      = {2010},
  isbn      = {9781605585772},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/1755913.1755940},
  doi       = {10.1145/1755913.1755940},
  abstract  = {As organizations start to use data-intensive cluster computing systems like Hadoop and Dryad for more applications, there is a growing need to share clusters between users. However, there is a conflict between fairness in scheduling and data locality (placing tasks on nodes that contain their input data). We illustrate this problem through our experience designing a fair scheduler for a 600-node Hadoop cluster at Facebook. To address the conflict between locality and fairness, we propose a simple algorithm called delay scheduling: when the job that should be scheduled next according to fairness cannot launch a local task, it waits for a small amount of time, letting other jobs launch tasks instead. We find that delay scheduling achieves nearly optimal data locality in a variety of workloads and can increase throughput by up to 2x while preserving fairness. In addition, the simplicity of delay scheduling makes it applicable under a wide variety of scheduling policies beyond fair sharing.},
  booktitle = {Proceedings of the 5th European Conference on Computer Systems},
  pages     = {265–278},
  numpages  = {14},
  keywords  = {mapreduce, cluster computing, scheduling, fair sharing},
  location  = {Paris, France},
  series    = {EuroSys '10}
}

@inproceedings{Sparrow,
  author    = {Ousterhout, Kay and Wendell, Patrick and Zaharia, Matei and Stoica, Ion},
  title     = {Sparrow: Distributed, Low Latency Scheduling},
  year      = {2013},
  isbn      = {9781450323888},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/2517349.2522716},
  doi       = {10.1145/2517349.2522716},
  abstract  = {Large-scale data analytics frameworks are shifting towards shorter task durations and larger degrees of parallelism to provide low latency. Scheduling highly parallel jobs that complete in hundreds of milliseconds poses a major challenge for task schedulers, which will need to schedule millions of tasks per second on appropriate machines while offering millisecond-level latency and high availability. We demonstrate that a decentralized, randomized sampling approach provides near-optimal performance while avoiding the throughput and availability limitations of a centralized design. We implement and deploy our scheduler, Sparrow, on a 110-machine cluster and demonstrate that Sparrow performs within 12% of an ideal scheduler.},
  booktitle = {Proceedings of the Twenty-Fourth ACM Symposium on Operating Systems Principles},
  pages     = {69–84},
  numpages  = {16},
  location  = {Farminton, Pennsylvania},
  series    = {SOSP '13}
}

@inproceedings{Spark,
  title     = {Resilient distributed datasets: A fault-tolerant abstraction for in-memory cluster computing},
  author    = {Zaharia, Matei and Chowdhury, Mosharaf and Das, Tathagata and Dave, Ankur and Ma, Justin and McCauly, Murphy and Franklin, Michael J and Shenker, Scott and Stoica, Ion},
  booktitle = {Presented as part of the 9th $\{$USENIX$\}$ Symposium on Networked Systems Design and Implementation ($\{$NSDI$\}$ 12)},
  pages     = {15--28},
  year      = {2012}
}

@misc{Hadoop,
  author       = {},
  title        = {Apache Hadoop},
  howpublished = {\url{https://hadoop.apache.org/}},
  month        = {},
  year         = {},
  note         = {(Accessed on 12/25/2020)}
}

@incollection{Decima,
  title     = {Learning scheduling algorithms for data processing clusters},
  author    = {Mao, Hongzi and Schwarzkopf, Malte and Venkatakrishnan, Shaileshh Bojja and Meng, Zili and Alizadeh, Mohammad},
  booktitle = {Proceedings of the ACM Special Interest Group on Data Communication},
  pages     = {270--288},
  year      = {2019}
}

@inproceedings{Philly,
  title     = {Analysis of large-scale multi-tenant $\{$GPU$\}$ clusters for $\{$DNN$\}$ training workloads},
  author    = {Jeon, Myeongjae and Venkataraman, Shivaram and Phanishayee, Amar and Qian, Junjie and Xiao, Wencong and Yang, Fan},
  booktitle = {2019 $\{$USENIX$\}$ Annual Technical Conference ($\{$USENIX$\}$$\{$ATC$\}$ 19)},
  pages     = {947--960},
  year      = {2019}
}