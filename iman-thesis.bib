@article{TPU,
  title   = {Google supercharges machine learning tasks with TPU custom chip},
  author  = {Jouppi, Norm},
  journal = {Google Blog, May},
  volume  = {18},
  pages   = {1},
  year    = {2016}
}

@misc{Volta,
  title        = {Volta Architecutre},
  howpublished = {\url{https://images.nvidia.com/content/volta-architecture/pdf/volta-architecture-whitepaper.pdf}},
  note         = {(Accessed on 10/20/2020)}
}

@misc{Occupancy,
  author       = {},
  title        = {CUDA Warps and Occupancy},
  howpublished = {\url{https://on-demand.gputechconf.com/gtc-express/2011/presentations/cuda_webinars_WarpsAndOccupancy.pdf}},
  month        = {},
  year         = {},
  note         = {(Accessed on 11/07/2020)}
}

@inproceedings{Tiresias,
  author    = {Juncheng Gu and Mosharaf Chowdhury and Kang G. Shin and Yibo Zhu and Myeongjae Jeon and Junjie Qian and Hongqiang Liu and Chuanxiong Guo},
  title     = {Tiresias: A {GPU} Cluster Manager for Distributed Deep Learning},
  booktitle = {16th {USENIX} Symposium on Networked Systems Design and Implementation ({NSDI} 19)},
  year      = {2019},
  isbn      = {978-1-931971-49-2},
  address   = {Boston, MA},
  pages     = {485--500},
  url       = {https://www.usenix.org/conference/nsdi19/presentation/gu},
  publisher = {{USENIX} Association},
  month     = feb
}

@inproceedings{Optimus,
  author    = {Peng, Yanghua and Bao, Yixin and Chen, Yangrui and Wu, Chuan and Guo, Chuanxiong},
  title     = {Optimus: An Efficient Dynamic Resource Scheduler for Deep Learning Clusters},
  year      = {2018},
  isbn      = {9781450355841},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3190508.3190517},
  doi       = {10.1145/3190508.3190517},
  abstract  = {Deep learning workloads are common in today's production clusters due to the proliferation of deep learning driven AI services (e.g., speech recognition, machine translation). A deep learning training job is resource-intensive and time-consuming. Efficient resource scheduling is the key to the maximal performance of a deep learning cluster. Existing cluster schedulers are largely not tailored to deep learning jobs, and typically specifying a fixed amount of resources for each job, prohibiting high resource efficiency and job performance. This paper proposes Optimus, a customized job scheduler for deep learning clusters, which minimizes job training time based on online resource-performance models. Optimus uses online fitting to predict model convergence during training, and sets up performance models to accurately estimate training speed as a function of allocated resources in each job. Based on the models, a simple yet effective method is designed and used for dynamically allocating resources and placing deep learning tasks to minimize job completion time. We implement Optimus on top of Kubernetes, a cluster manager for container orchestration, and experiment on a deep learning cluster with 7 CPU servers and 6 GPU servers, running 9 training jobs using the MXNet framework. Results show that Optimus outperforms representative cluster schedulers by about 139% and 63% in terms of job completion time and makespan, respectively.},
  booktitle = {Proceedings of the Thirteenth EuroSys Conference},
  articleno = {3},
  numpages  = {14},
  keywords  = {deep learning, resource management},
  location  = {Porto, Portugal},
  series    = {EuroSys '18}
}

@inbook{SLAQ,
  author    = {Zhang, Haoyu and Stafman, Logan and Or, Andrew and Freedman, Michael J.},
  title     = {SLAQ: Quality-Driven Scheduling for Distributed Machine Learning},
  year      = {2017},
  isbn      = {9781450350280},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3127479.3127490},
  abstract  = {Training machine learning (ML) models with large datasets can incur significant resource contention on shared clusters. This training typically involves many iterations that continually improve the quality of the model. Yet in exploratory settings, better models can be obtained faster by directing resources to jobs with the most potential for improvement. We describe SLAQ, a cluster scheduling system for approximate ML training jobs that aims to maximize the overall job quality.When allocating cluster resources, SLAQ explores the quality-runtime trade-offs across multiple jobs to maximize system-wide quality improvement. To do so, SLAQ leverages the iterative nature of ML training algorithms, by collecting quality and resource usage information from concurrent jobs, and then generating highly-tailored quality-improvement predictions for future iterations. Experiments show that SLAQ achieves an average quality improvement of up to 73% and an average delay reduction of up to 44% on a large set of ML training jobs, compared to resource fairness schedulers.},
  booktitle = {Proceedings of the 2017 Symposium on Cloud Computing},
  pages     = {390â€“404},
  numpages  = {15}
}

@inproceedings{ParameterServer,
  author    = {Mu Li and David G. Andersen and Jun Woo Park and Alexander J. Smola and Amr Ahmed and Vanja Josifovski and James Long and Eugene J. Shekita and Bor-Yiing Su},
  title     = {Scaling Distributed Machine Learning with the Parameter Server},
  booktitle = {11th {USENIX} Symposium on Operating Systems Design and Implementation ({OSDI} 14)},
  year      = {2014},
  isbn      = { 978-1-931971-16-4},
  address   = {Broomfield, CO},
  pages     = {583--598},
  url       = {https://www.usenix.org/conference/osdi14/technical-sessions/presentation/li_mu},
  publisher = {{USENIX} Association},
  month     = oct
}

@inproceedings{Gandiva,
  author    = {Wencong Xiao and Romil Bhardwaj and Ramachandran Ramjee and Muthian Sivathanu and Nipun Kwatra and Zhenhua Han and Pratyush Patel and Xuan Peng and Hanyu Zhao and Quanlu Zhang and Fan Yang and Lidong Zhou},
  title     = {Gandiva: Introspective Cluster Scheduling for Deep Learning},
  booktitle = {13th {USENIX} Symposium on Operating Systems Design and Implementation ({OSDI} 18)},
  year      = {2018},
  isbn      = {978-1-939133-08-3},
  address   = {Carlsbad, CA},
  pages     = {595--610},
  url       = {https://www.usenix.org/conference/osdi18/presentation/xiao},
  publisher = {{USENIX} Association},
  month     = oct
}